{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from model import CreditModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.loadtxt('data/credit-data.csv', dtype=np.int, delimiter=',', skiprows=1)\n",
    "X, y = data[:, 1:-1], data[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguements:\n",
    "        Z -- input for activation\n",
    "    \n",
    "    returns:\n",
    "        A -- result of activation function\n",
    "        cache -- store Z for back prop\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA*s*(1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguements:\n",
    "        Z -- input for activation\n",
    "    \n",
    "    returns:\n",
    "        A -- result of activation function\n",
    "        cache -- store Z for back prop\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    cache = Z\n",
    "    \n",
    "    return A , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_back(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(layerdims):\n",
    "    \"\"\"\n",
    "    Arguements:\n",
    "    layerdims = python array of dimension of each layer of network\n",
    "    \n",
    "    returns:\n",
    "    params = python dictionary with: \"W1\", \"b2\"\n",
    "        W1 -- weight matrix of shape (layer_dims[1], layer_dims[l-1])\n",
    "        bl -- bias vector of shape (layer_dims[l], 1)\n",
    "\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    L = len(layerdims)\n",
    "    \n",
    "    for l in range (1,L):\n",
    "        params[\"W\" + str(l)] = np.random.randn(layerdims[l], layerdims[l-1]) * 0.01\n",
    "        params[\"b\" + str(l)] = np.zeros((layerdims[l],1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_prop(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguement:\n",
    "        A is activation from previous layer\n",
    "        W is weight of current layer\n",
    "        b is bias vector of current layer\n",
    "        \n",
    "   returns:\n",
    "       Z -- the activation input\n",
    "       cache -- stored A,W,b for better computation later   \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))    \n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_act_forward(A_prev, W,b, act):\n",
    "    \"\"\"\n",
    "    Arguement:\n",
    "        A_prev -- activation from previous layer\n",
    "        W -- weights of current layer\n",
    "        b -- bias vector of current layer\n",
    "        act -- activation choice for layer\n",
    "    \n",
    "    return:\n",
    "        A -- activation of current layer\n",
    "        cache -- dictionary with lin_cache and act_cache\n",
    "    \"\"\"\n",
    "    if act == \"relu\":\n",
    "        Z, lin_cache = linear_prop(A_prev, W, b)\n",
    "        A, act_cache = relu(Z)\n",
    "    \n",
    "    elif act == \"sigmoid\":\n",
    "        Z, lin_cache = linear_prop(A_prev,W,b)\n",
    "        A, act_cache = sigmoid(Z)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))    \n",
    "    \n",
    "    cache = (lin_cache, act_cache)\n",
    "    \n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_forward(X,params):\n",
    "    \"\"\"\n",
    "    Arguement:\n",
    "        X -- data\n",
    "        params -- initialized params\n",
    "    return:\n",
    "        AL -- last activation values\n",
    "        caches -- all activation values (there is L-1 of them)   \n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(params)//2   # number of layers\n",
    "    A = X\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        prev_A = A\n",
    "        A, cache = linear_act_forward(prev_A, params[\"W\" + str(l)],params[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_act_forward(A, params[\"W\" + str(L)], params[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "\n",
    "    return AL, caches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "\n",
    "    cost = (-1/m)*np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL)))\n",
    "\n",
    "    \n",
    "    cost.shape\n",
    "    cost = np.squeeze(cost)      \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41493159961539694"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    Y = np.asarray([[1, 1, 1]])\n",
    "    aL = np.array([[.8,.9,0.4]])\n",
    "    cost(aL, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_back(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def act_back(dA, cache, act):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if act == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = lin_back(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif act == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = lin_back(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_back(AL,Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = np.divide(-Y,AL) + np.divide((1-Y),1-AL)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = act_back(dAL,current_cache,\"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = act_back(grads[\"dA\" + str(l+1)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l + 1)] = params[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(X_train):\n",
    "\n",
    "    X = np.copy(X_train)\n",
    "    X_n = { \"X_p1\" : np.reshape(X[:,0]/np.amax(X[:,0]),(-1,1)),\n",
    "            \"X_p2\" : X[:, 1:4],\n",
    "            \"X_p3\" : np.reshape(X[:,4]/np.amax(X[:,1]),(-1,1)),\n",
    "            \"X_p4\" : X[:,5:11] / 12,\n",
    "            \"X_p5\" : X[:, 11:17] / np.amax(X[:, 11:17]),\n",
    "            \"X_p6\" : X[:,17:23] / np.amax(X[:,17:22])}\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    X_new = X_n[\"X_p1\"]\n",
    "    \n",
    "    \n",
    "    for i in range(2, 7):\n",
    "        X_new = np.concatenate((X_new,X_n[\"X_p\" + str(i)] ),axis = 1)\n",
    "    \n",
    "    return X_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(X_train, y_train):\n",
    "    X_temp = X_train[0,:]\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len(y_train)):\n",
    "        if 1 == y_train[i]:\n",
    "            X_temp = np.concatenate((X_temp, X_train[i,:]), axis = 0)\n",
    "            k += 1\n",
    "    X_temp = np.reshape(X_temp, (np.sum(y_train)+1,-1))\n",
    "    y_temp = np.ones((X_temp.shape[1],1))\n",
    "    return X_temp, y_temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train,  y_train):\n",
    "\n",
    "    #len(y_train) full sample size = 22500\n",
    "    X_train_t, y_train_t  = batch(X_train,y_train)\n",
    "    \n",
    "    sample_true = 0\n",
    "    sample_size_used = 2500\n",
    "    \n",
    "    X_train_tn = X_train_t[0:sample_true,:]\n",
    "    X_trainn = X_train[0:sample_size_used,:]\n",
    "    \n",
    "    X_n = np.concatenate((X_train_tn, X_trainn), axis = 0)\n",
    "\n",
    "    X_normalized = normalize_data(X_n)\n",
    "    X_n = X_normalized.T\n",
    "    \n",
    "    y_train = y_train[0:sample_size_used]\n",
    "    y_train = np.concatenate((np.ones((sample_true,1)),np.reshape(y_train,(-1,1))),axis = 0)\n",
    "    \n",
    "    \n",
    "\n",
    "    print_cost= True\n",
    "\n",
    "    layersdims = (X_train.shape[1], 20,7,5,1)\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    params = init_params(layersdims)\n",
    "\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "  \n",
    "\n",
    "        AL, caches = compute_forward(X_n,params)\n",
    "        if i == 1:\n",
    "            print(AL)\n",
    "        cost_s = cost(AL,y_train)\n",
    "\n",
    "        grads = nn_back(AL,y_train,caches)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        params = update_params(params, grads, learning_rate)\n",
    "        \n",
    "\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost_s))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_s)\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 4332170.535332\n",
      "[[ 0.49305048  0.49305045  0.49305047 ...,  0.4930505   0.49305048\n",
      "   0.49305045]]\n",
      "Cost after iteration 100: 3325508.564440\n",
      "Cost after iteration 200: 3309380.997701\n",
      "Cost after iteration 300: 3308936.702058\n",
      "Cost after iteration 400: 3308923.324372\n",
      "Cost after iteration 500: 3308922.915405\n",
      "Cost after iteration 600: 3308922.902870\n",
      "Cost after iteration 700: 3308922.902485\n",
      "Cost after iteration 800: 3308922.902474\n",
      "Cost after iteration 900: 3308922.902473\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEWCAYAAADPZygPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnoVFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1278107c860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solution = model(X_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(sol,X_test):\n",
    "    X_test = normalize_data(X_test)\n",
    "    X_test = X_test.T\n",
    "    AL, caches = compute_forward(X_test,sol)\n",
    "    \n",
    "    return np.round(AL.T)\n",
    "        \n",
    "    \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = predict(solution,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkright(y_hat, y_test):\n",
    "    y = np.reshape(y_test, (-1,1))\n",
    "    ls = np.ndarray.tolist(y == y_hat)\n",
    "    k = 0\n",
    "    print(y == y_hat)\n",
    "    for i in ls:\n",
    "        if k< 3:\n",
    "            print(i)\n",
    "        if i:\n",
    "            k = k + 1\n",
    "    \n",
    "    print(k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
